{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0c7c6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9130e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f3ce0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74b9f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_API_URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "DEFAULT_IMAGE = r\"HighResImage/pexels-hikaique-109919.jpg\"\n",
    "MAX_SIZE = (896, 896)\n",
    "JPEG_QUALITY = 85\n",
    "DEFAULT_MODEL = os.environ.get(\"LM_MODEL\", \"qwen/qwen3-vl-8b\")\n",
    "# ----------------------------------\n",
    "\n",
    "OUTPUT_JSON_TEMPLATE = {\n",
    "    \"source_image\": None,\n",
    "    \"timestamp\": None,\n",
    "    \"clip_caption\": None,\n",
    "    \"blip_description\": None,\n",
    "    \"prompt_used\": None,\n",
    "    \"lm_response_raw\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e350ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "336301b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_text(filename: str, width: int, height: int) -> str:\n",
    "    \"\"\"\n",
    "    Prompt curto que pede ao modelo para detectar todas as pessoas na imagem\n",
    "    e retornar um JSON com um caption global e uma lista 'people' com\n",
    "    clip_caption (1-6 words) e blip_description (1-2 sentences) para cada pessoa.\n",
    "    Também pede um campo bbox normalizado [x1,y1,x2,y2] se o modelo puder fornecer,\n",
    "    caso contrário colocar null.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Analyze the image and detect every person visible. \"\n",
    "        \"Respond ONLY with a single JSON object (no prose) with exactly two top-level fields:\\n\"\n",
    "        \"1) \\\"global\\\": a very short caption for the whole image (1-6 words, label-style, lower-case).\\n\"\n",
    "        \"2) \\\"people\\\": an array of objects, one per detected person, each with the fields:\\n\"\n",
    "        \"   - \\\"id\\\": integer (1..N)\\n\"\n",
    "        \"   - \\\"clip_caption\\\": 1-6 words, label-style, lower-case, no trailing punctuation\\n\"\n",
    "        \"   - \\\"blip_description\\\": 1-2 short sentences describing that person (clothing, color, action, pose, visible objects, relative location); avoid inventing identities or unverifiable facts\\n\"\n",
    "        \"   - \\\"bbox\\\": either an array [x1, y1, x2, y2] with coordinates normalized to the image width/height (values 0..1), OR null if you cannot provide coordinates\\n\"\n",
    "        \"Rules: do not invent names, ages, or identities. If unsure, use neutral phrasing (e.g. 'appears to be', 'possibly'). \"\n",
    "        \"Return only valid JSON. No additional text.\"\n",
    "    )\n",
    "\n",
    "def extract_json_from_response(resp: Dict[str, Any]) -> Optional[Dict[str,str]]:\n",
    "    \"\"\"\n",
    "    Heurística para extrair clip_caption e blip_description de várias formas de resposta.\n",
    "    \"\"\"\n",
    "    if not resp:\n",
    "        return None\n",
    "\n",
    "    # direct keys\n",
    "    if all(k in resp for k in (\"clip_caption\", \"blip_description\")):\n",
    "        return {\"clip_caption\": resp[\"clip_caption\"], \"blip_description\": resp[\"blip_description\"]}\n",
    "\n",
    "    # common wrapper keys\n",
    "    for key in (\"response\", \"text\", \"output\", \"result\", \"choices\"):\n",
    "        if key in resp:\n",
    "            val = resp[key]\n",
    "            # choices array (OpenAI style)\n",
    "            if key == \"choices\" and isinstance(val, list) and len(val) > 0:\n",
    "                # try to extract message content\n",
    "                first = val[0]\n",
    "                if isinstance(first, dict):\n",
    "                    # openai chat-style\n",
    "                    msg = first.get(\"message\") or first.get(\"text\") or first.get(\"content\")\n",
    "                    if isinstance(msg, dict):\n",
    "                        # message -> content: could be string or list\n",
    "                        content = msg.get(\"content\")\n",
    "                        if isinstance(content, str):\n",
    "                            try:\n",
    "                                parsed = json.loads(content)\n",
    "                                if all(k in parsed for k in (\"clip_caption\", \"blip_description\")):\n",
    "                                    return {\"clip_caption\": parsed[\"clip_caption\"], \"blip_description\": parsed[\"blip_description\"]}\n",
    "                            except Exception:\n",
    "                                # fallthrough to regex parsing\n",
    "                                text = content\n",
    "                                return _extract_from_text(text)\n",
    "                    elif isinstance(msg, str):\n",
    "                        return _extract_from_text(msg)\n",
    "                # fallback: choice text field\n",
    "                if isinstance(first.get(\"text\"), str):\n",
    "                    return _extract_from_text(first.get(\"text\"))\n",
    "            # if val is a string\n",
    "            if isinstance(val, str):\n",
    "                return _extract_from_text(val)\n",
    "            # if val is dict, try nested\n",
    "            if isinstance(val, dict):\n",
    "                # try to find textual fields inside\n",
    "                for sub in (\"text\", \"output\", \"response\"):\n",
    "                    if sub in val and isinstance(val[sub], str):\n",
    "                        return _extract_from_text(val[sub])\n",
    "    # last attempt: search entire resp JSON string\n",
    "    try:\n",
    "        s = json.dumps(resp)\n",
    "        return _extract_from_text(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _extract_from_text(text: str) -> Optional[Dict[str,str]]:\n",
    "    \"\"\"Try to parse JSON from text or extract via regex.\"\"\"\n",
    "    import re\n",
    "    if not text:\n",
    "        return None\n",
    "    txt = text.strip()\n",
    "    # try parse JSON directly\n",
    "    try:\n",
    "        parsed = json.loads(txt)\n",
    "        if isinstance(parsed, dict) and all(k in parsed for k in (\"clip_caption\", \"blip_description\")):\n",
    "            return {\"clip_caption\": parsed[\"clip_caption\"], \"blip_description\": parsed[\"blip_description\"]}\n",
    "    except Exception:\n",
    "        pass\n",
    "    # regex extraction\n",
    "    m1 = re.search(r'\"clip_caption\"\\s*:\\s*\"([^\"]+)\"', txt)\n",
    "    m2 = re.search(r'\"blip_description\"\\s*:\\s*\"([^\"]+)\"', txt)\n",
    "    if m1 and m2:\n",
    "        return {\"clip_caption\": m1.group(1), \"blip_description\": m2.group(1)}\n",
    "    # try loose pattern: 'clip_caption: something' lines\n",
    "    lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]\n",
    "    # find lines containing clip_caption / blip_description\n",
    "    cc = None\n",
    "    bd = None\n",
    "    for ln in lines:\n",
    "        if \"clip_caption\" in ln and \":\" in ln:\n",
    "            try:\n",
    "                cc = ln.split(\":\",1)[1].strip().strip('\", ')\n",
    "            except:\n",
    "                pass\n",
    "        if \"blip_description\" in ln and \":\" in ln:\n",
    "            try:\n",
    "                bd = ln.split(\":\",1)[1].strip().strip('\", ')\n",
    "            except:\n",
    "                pass\n",
    "    if cc and bd:\n",
    "        return {\"clip_caption\": cc, \"blip_description\": bd}\n",
    "    return None\n",
    "\n",
    "def prepare_image_for_model(image_path: str, max_size=(896,896), mode: str = \"resize\",\n",
    "                            output_dir: Optional[str] = None, jpeg_quality: int = 85) -> str:\n",
    "    \"\"\"\n",
    "    Prepara imagem para o modelo (resize/crop/pad). Retorna caminho do arquivo salvo.\n",
    "    Default: resize mantendo aspecto, max dimension = max_size.\n",
    "    \"\"\"\n",
    "    if mode not in (\"resize\", \"crop\", \"pad\"):\n",
    "        raise ValueError(\"mode deve ser 'resize'|'crop'|'pad'\")\n",
    "\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    target_w, target_h = max_size\n",
    "\n",
    "    if mode == \"resize\":\n",
    "        im.thumbnail(max_size, Image.LANCZOS)\n",
    "        out_im = im\n",
    "    elif mode == \"crop\":\n",
    "        short = min(w,h)\n",
    "        left = (w - short)//2\n",
    "        top = (h - short)//2\n",
    "        right = left + short\n",
    "        bottom = top + short\n",
    "        im_cropped = im.crop((left, top, right, bottom))\n",
    "        out_im = im_cropped.resize(max_size, Image.LANCZOS)\n",
    "    else:  # pad\n",
    "        im.thumbnail(max_size, Image.LANCZOS)\n",
    "        out_im = Image.new(\"RGB\", max_size, (0,0,0))\n",
    "        paste_x = (target_w - im.width)//2\n",
    "        paste_y = (target_h - im.height)//2\n",
    "        out_im.paste(im, (paste_x, paste_y))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        base = os.path.basename(image_path)\n",
    "        name, _ = os.path.splitext(base)\n",
    "        out_path = os.path.join(output_dir, f\"{name}_prepared_{target_w}x{target_h}.jpg\")\n",
    "    else:\n",
    "        fd, out_path = tempfile.mkstemp(suffix=f\"_{target_w}x{target_h}.jpg\")\n",
    "        os.close(fd)\n",
    "\n",
    "    out_im.save(out_path, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "def send_to_local_api(prompt_text: str, api_url: str, image_path: Optional[str] = None, model_name: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Envia payload para /v1/chat/completions com:\n",
    "      messages: [{role:'user', content: [{type:'input_text', 'text':...}, {type:'input_image','image_url':{'url':'data:image/jpeg;base64,...'}}]}]\n",
    "    Observação: pode gerar payload grande — já resize a imagem antes (896x896).\n",
    "    \"\"\"\n",
    "    import os, json, base64\n",
    "    if requests is None:\n",
    "        print(\"requests não instalado.\")\n",
    "        return None\n",
    "\n",
    "    model = model_name or DEFAULT_MODEL\n",
    "\n",
    "    # montar blocos de conteúdo\n",
    "    content_blocks = [{\"type\": \"input_text\", \"text\": prompt_text}]\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        # converter a imagem para base64 (data URL)\n",
    "        with open(image_path, \"rb\") as fh:\n",
    "            b64 = base64.b64encode(fh.read()).decode(\"ascii\")\n",
    "        data_url = f\"data:image/jpeg;base64,{b64}\"\n",
    "        content_blocks.append({\"type\": \"input_image\", \"image_url\": {\"url\": data_url}})\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_blocks\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # usar chat/completions (se quiser testar /responses, precisa saber esquema exato do servidor)\n",
    "    try:\n",
    "        print(\">>> POST /v1/chat/completions payload (truncated):\")\n",
    "        print(json.dumps(payload, ensure_ascii=False)[:1000])\n",
    "        resp = requests.post(api_url, json=payload, timeout=300)  # timeout maior para uploads grandes\n",
    "        print(\"status:\", resp.status_code)\n",
    "        print(\"body (start):\", resp.text[:4000])\n",
    "        resp.raise_for_status()\n",
    "        try:\n",
    "            return resp.json()\n",
    "        except Exception:\n",
    "            return {\"text\": resp.text}\n",
    "    except Exception as e:\n",
    "        print(\"Request failed:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd37a73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a65edd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    image_path: str = DEFAULT_IMAGE,\n",
    "    api_url: str = LOCAL_API_URL,\n",
    "    out_path: str = \"image_caption_blip_clip.json\",\n",
    "    auto_send: bool = True,\n",
    "):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\"Arquivo não encontrado:\", image_path)\n",
    "        return\n",
    "\n",
    "    # Prepare image (resize) and build short prompt text (no base64)\n",
    "    prepared_path = prepare_image_for_model(image_path, max_size=MAX_SIZE, mode=\"resize\", jpeg_quality=JPEG_QUALITY)\n",
    "    im = Image.open(prepared_path)\n",
    "    w, h = im.size\n",
    "    prompt_text = build_prompt_text(os.path.basename(prepared_path), w, h)\n",
    "\n",
    "    output = OUTPUT_JSON_TEMPLATE.copy()\n",
    "    output[\"source_image\"] = os.path.abspath(prepared_path)\n",
    "    output[\"timestamp\"] = datetime.datetime.now().isoformat()\n",
    "    output[\"prompt_used\"] = prompt_text\n",
    "\n",
    "    lm_resp = None\n",
    "    parsed = None\n",
    "\n",
    "    if auto_send:\n",
    "        print(f\"Tentando enviar prompt para {api_url} ...\")\n",
    "        lm_resp = send_to_local_api(prompt_text, api_url, image_path=prepared_path)\n",
    "        output[\"lm_response_raw\"] = lm_resp\n",
    "        parsed = extract_json_from_response(lm_resp if isinstance(lm_resp, dict) else {})\n",
    "        if parsed:\n",
    "            output[\"clip_caption\"] = parsed[\"clip_caption\"]\n",
    "            output[\"blip_description\"] = parsed[\"blip_description\"]\n",
    "            print(\"Resposta extraída com sucesso do endpoint.\")\n",
    "        else:\n",
    "            print(\"Não foi possível extrair JSON estruturado da resposta automática.\")\n",
    "            print(\"Se falhar, rode em modo manual (auto_send=False) e cole o prompt no UI do LM Studio.\")\n",
    "    else:\n",
    "        print(\"Modo manual: cole o prompt curto e a imagem (prepared) no UI do LM Studio:\")\n",
    "        print(\"Prompt:\\n\", prompt_text)\n",
    "        print(\"Prepared image path (use file:/// in UI if supported):\", prepared_path)\n",
    "        # allow interactive paste of the model JSON response\n",
    "        try:\n",
    "            raw = input(\"\\nCole aqui o JSON resposta do modelo (ou pressione Enter para pular):\\n\").strip()\n",
    "            if raw:\n",
    "                j = json.loads(raw)\n",
    "                if \"clip_caption\" in j and \"blip_description\" in j:\n",
    "                    output[\"clip_caption\"] = j[\"clip_caption\"]\n",
    "                    output[\"blip_description\"] = j[\"blip_description\"]\n",
    "                    output[\"lm_response_raw\"] = j\n",
    "                    print(\"Resposta manual salva.\")\n",
    "                else:\n",
    "                    print(\"JSON colado não possui as chaves esperadas.\")\n",
    "        except Exception as e:\n",
    "            print(\"Erro ao ler entrada manual:\", e)\n",
    "\n",
    "    # Save JSON (even if partial)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_path)) or \".\", exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(output, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\nArquivo salvo em:\", out_path)\n",
    "    if output.get(\"clip_caption\"):\n",
    "        print(\"clip_caption:\", output[\"clip_caption\"])\n",
    "    if output.get(\"blip_description\"):\n",
    "        print(\"blip_description:\", output[\"blip_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f413b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2982c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando enviar prompt para http://localhost:1234/v1/chat/completions ...\n",
      ">>> POST /v1/chat/completions payload (truncated):\n",
      "{\"model\": \"qwen/qwen3-vl-8b\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": \"Analyze the image and detect every person visible. Respond ONLY with a single JSON object (no prose) with exactly two top-level fields:\\n1) \\\"global\\\": a very short caption for the whole image (1-6 words, label-style, lower-case).\\n2) \\\"people\\\": an array of objects, one per detected person, each with the fields:\\n   - \\\"id\\\": integer (1..N)\\n   - \\\"clip_caption\\\": 1-6 words, label-style, lower-case, no trailing punctuation\\n   - \\\"blip_description\\\": 1-2 short sentences describing that person (clothing, color, action, pose, visible objects, relative location); avoid inventing identities or unverifiable facts\\n   - \\\"bbox\\\": either an array [x1, y1, x2, y2] with coordinates normalized to the image width/height (values 0..1), OR null if you cannot provide coordinates\\nRules: do not invent names, ages, or identities. If unsure, use neutral phrasing (e.g. 'appears to be', 'possibly'). R\n",
      "status: 200\n",
      "body (start): {\n",
      "  \"id\": \"chatcmpl-syopcldq19l3jq2zbddfoc\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1763042014,\n",
      "  \"model\": \"qwen/qwen3-vl-8b\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"{\\n  \\\"global\\\": \\\"people crossing city street\\\",\\n  \\\"people\\\": [\\n    {\\n      \\\"id\\\": 1,\\n      \\\"clip_caption\\\": \\\"man with glasses walking\\\",\\n      \\\"blip_description\\\": \\\"man wearing black jacket and shorts walking across street. appears to be carrying water bottle.\\\",\\n      \\\"bbox\\\": [0.12, 0.31, 0.32, 0.82]\\n    },\\n    {\\n      \\\"id\\\": 2,\\n      \\\"clip_caption\\\": \\\"woman in white hoodie\\\",\\n      \\\"blip_description\\\": \\\"woman wearing white hoodie and black pants crossing street. carrying patterned bag.\\\",\\n      \\\"bbox\\\": [0.38, 0.4, 0.5, 0.83]\\n    },\\n    {\\n      \\\"id\\\": 3,\\n      \\\"clip_caption\\\": \\\"man in white t-shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing white t-shirt and blue jeans walking across street. holding something in hand.\\\",\\n      \\\"bbox\\\": [0.51, 0.37, 0.63, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 4,\\n      \\\"clip_caption\\\": \\\"woman in black coat\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black coat and dark pants walking across street. holding bag.\\\",\\n      \\\"bbox\\\": [0.61, 0.39, 0.7, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 5,\\n      \\\"clip_caption\\\": \\\"woman in grey jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing grey jacket and dark pants walking across street. carrying black bag.\\\",\\n      \\\"bbox\\\": [0.75, 0.41, 0.85, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 6,\\n      \\\"clip_caption\\\": \\\"man in grey shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing grey shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.91, 0.38, 1.0, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 7,\\n      \\\"clip_caption\\\": \\\"man in dark jacket\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.3, 0.4, 0.38, 0.71]\\n    },\\n    {\\n      \\\"id\\\": 8,\\n      \\\"clip_caption\\\": \\\"woman in brown jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing brown jacket and dark pants walking across street. carrying bag.\\\",\\n      \\\"bbox\\\": [0.78, 0.41, 0.87, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 9,\\n      \\\"clip_caption\\\": \\\"woman in black jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.67, 0.39, 0.75, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 10,\\n      \\\"clip_caption\\\": \\\"woman in red jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing red jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.85, 0.41, 0.92, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 11,\\n      \\\"clip_caption\\\": \\\"man in dark shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.58, 0.38, 0.66, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 12,\\n      \\\"clip_caption\\\": \\\"woman in light jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing light jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.71, 0.39, 0.79, 0.77]\\n    }\\n  ]\\n}\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 243,\n",
      "    \"completion_tokens\": 860,\n",
      "    \"total_tokens\": 1103\n",
      "  },\n",
      "  \"stats\": {},\n",
      "  \"system_fingerprint\": \"qwen/qwen3-vl-8b\"\n",
      "}\n",
      "Resposta extraída com sucesso do endpoint.\n",
      "\n",
      "Arquivo salvo em: image_caption_blip_clip.json\n",
      "clip_caption: chatcmpl-syopcldq19l3jq2zbddfoc\", \"object\": \"chat.completion\", \"created\": 1763042014, \"model\": \"qwen/qwen3-vl-8b\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"global\\\": \\\"people crossing city street\\\",\\n  \\\"people\\\": [\\n    {\\n      \\\"id\\\": 1,\\n      \\\"clip_caption\\\": \\\"man with glasses walking\\\",\\n      \\\"blip_description\\\": \\\"man wearing black jacket and shorts walking across street. appears to be carrying water bottle.\\\",\\n      \\\"bbox\\\": [0.12, 0.31, 0.32, 0.82]\\n    },\\n    {\\n      \\\"id\\\": 2,\\n      \\\"clip_caption\\\": \\\"woman in white hoodie\\\",\\n      \\\"blip_description\\\": \\\"woman wearing white hoodie and black pants crossing street. carrying patterned bag.\\\",\\n      \\\"bbox\\\": [0.38, 0.4, 0.5, 0.83]\\n    },\\n    {\\n      \\\"id\\\": 3,\\n      \\\"clip_caption\\\": \\\"man in white t-shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing white t-shirt and blue jeans walking across street. holding something in hand.\\\",\\n      \\\"bbox\\\": [0.51, 0.37, 0.63, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 4,\\n      \\\"clip_caption\\\": \\\"woman in black coat\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black coat and dark pants walking across street. holding bag.\\\",\\n      \\\"bbox\\\": [0.61, 0.39, 0.7, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 5,\\n      \\\"clip_caption\\\": \\\"woman in grey jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing grey jacket and dark pants walking across street. carrying black bag.\\\",\\n      \\\"bbox\\\": [0.75, 0.41, 0.85, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 6,\\n      \\\"clip_caption\\\": \\\"man in grey shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing grey shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.91, 0.38, 1.0, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 7,\\n      \\\"clip_caption\\\": \\\"man in dark jacket\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.3, 0.4, 0.38, 0.71]\\n    },\\n    {\\n      \\\"id\\\": 8,\\n      \\\"clip_caption\\\": \\\"woman in brown jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing brown jacket and dark pants walking across street. carrying bag.\\\",\\n      \\\"bbox\\\": [0.78, 0.41, 0.87, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 9,\\n      \\\"clip_caption\\\": \\\"woman in black jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.67, 0.39, 0.75, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 10,\\n      \\\"clip_caption\\\": \\\"woman in red jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing red jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.85, 0.41, 0.92, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 11,\\n      \\\"clip_caption\\\": \\\"man in dark shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.58, 0.38, 0.66, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 12,\\n      \\\"clip_caption\\\": \\\"woman in light jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing light jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.71, 0.39, 0.79, 0.77]\\n    }\\n  ]\\n}\", \"tool_calls\": []}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 243, \"completion_tokens\": 860, \"total_tokens\": 1103}, \"stats\": {}, \"system_fingerprint\": \"qwen/qwen3-vl-8b\"}\n",
      "blip_description: chatcmpl-syopcldq19l3jq2zbddfoc\", \"object\": \"chat.completion\", \"created\": 1763042014, \"model\": \"qwen/qwen3-vl-8b\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"global\\\": \\\"people crossing city street\\\",\\n  \\\"people\\\": [\\n    {\\n      \\\"id\\\": 1,\\n      \\\"clip_caption\\\": \\\"man with glasses walking\\\",\\n      \\\"blip_description\\\": \\\"man wearing black jacket and shorts walking across street. appears to be carrying water bottle.\\\",\\n      \\\"bbox\\\": [0.12, 0.31, 0.32, 0.82]\\n    },\\n    {\\n      \\\"id\\\": 2,\\n      \\\"clip_caption\\\": \\\"woman in white hoodie\\\",\\n      \\\"blip_description\\\": \\\"woman wearing white hoodie and black pants crossing street. carrying patterned bag.\\\",\\n      \\\"bbox\\\": [0.38, 0.4, 0.5, 0.83]\\n    },\\n    {\\n      \\\"id\\\": 3,\\n      \\\"clip_caption\\\": \\\"man in white t-shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing white t-shirt and blue jeans walking across street. holding something in hand.\\\",\\n      \\\"bbox\\\": [0.51, 0.37, 0.63, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 4,\\n      \\\"clip_caption\\\": \\\"woman in black coat\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black coat and dark pants walking across street. holding bag.\\\",\\n      \\\"bbox\\\": [0.61, 0.39, 0.7, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 5,\\n      \\\"clip_caption\\\": \\\"woman in grey jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing grey jacket and dark pants walking across street. carrying black bag.\\\",\\n      \\\"bbox\\\": [0.75, 0.41, 0.85, 0.79]\\n    },\\n    {\\n      \\\"id\\\": 6,\\n      \\\"clip_caption\\\": \\\"man in grey shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing grey shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.91, 0.38, 1.0, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 7,\\n      \\\"clip_caption\\\": \\\"man in dark jacket\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.3, 0.4, 0.38, 0.71]\\n    },\\n    {\\n      \\\"id\\\": 8,\\n      \\\"clip_caption\\\": \\\"woman in brown jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing brown jacket and dark pants walking across street. carrying bag.\\\",\\n      \\\"bbox\\\": [0.78, 0.41, 0.87, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 9,\\n      \\\"clip_caption\\\": \\\"woman in black jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing black jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.67, 0.39, 0.75, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 10,\\n      \\\"clip_caption\\\": \\\"woman in red jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing red jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.85, 0.41, 0.92, 0.77]\\n    },\\n    {\\n      \\\"id\\\": 11,\\n      \\\"clip_caption\\\": \\\"man in dark shirt\\\",\\n      \\\"blip_description\\\": \\\"man wearing dark shirt and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.58, 0.38, 0.66, 0.78]\\n    },\\n    {\\n      \\\"id\\\": 12,\\n      \\\"clip_caption\\\": \\\"woman in light jacket\\\",\\n      \\\"blip_description\\\": \\\"woman wearing light jacket and dark pants walking across street.\\\",\\n      \\\"bbox\\\": [0.71, 0.39, 0.79, 0.77]\\n    }\\n  ]\\n}\", \"tool_calls\": []}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 243, \"completion_tokens\": 860, \"total_tokens\": 1103}, \"stats\": {}, \"system_fingerprint\": \"qwen/qwen3-vl-8b\"}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\"HighResImage/pexels-hikaique-109919.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
