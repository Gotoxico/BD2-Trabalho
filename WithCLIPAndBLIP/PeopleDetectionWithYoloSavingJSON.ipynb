{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9051e94",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4decdf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\OneDrive - Unesp\\Documentos\\GitHub\\BD2-Trabalho\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Detector: ultralytics YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# BLIP captioning do Hugging Face / transformers\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "# CLIP (OpenAI)\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bc717",
   "metadata": {},
   "source": [
    "Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c76e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INPUT_FRAMES_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Frames\" #Caminho da pasta com os frames extraídos dos vídeos em VideoResizeAndFrameExtractionPrincipal.ipynb, alterar para sua pasta\n",
    "OUTPUT_CROPS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/CroppedPersons2\" #Caminho da pasta para salvar os crops dos indivíduos/grupos, alterar para sua pasta            \n",
    "OUTPUT_RECORDS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Records2\" #Caminho da pasta para salvar os records únicos para cada ID, alterar para sua pasta  '''\n",
    "\n",
    "INPUT_FRAMES_FOLDER = \"../HighResImage\" \n",
    "OUTPUT_CROPS_FOLDER = \"CroppedPersonsHighRes\"\n",
    "OUTPUT_RECORDS_FOLDER = \"RecordsHighRes\"\n",
    "\n",
    "YOLO_WEIGHTS = \"../yolo11n.pt\" #Idealmente utilizar a versão mais recente e a \"n\", pois é mais leve (precisa baixar no site da Ultralytics)\n",
    "YOLO_CONF = 0.35                           \n",
    "CAMERA_ID = 1\n",
    "\n",
    "IOU_MATCH_THRESHOLD = 0.3                 \n",
    "MIN_BOX_AREA = 400                         \n",
    "ATTR_TOP_K = 2\n",
    "ATTR_CONF_THRESHOLD = 0.05\n",
    "\n",
    "# BLIP model (Hugging Face)\n",
    "BLIP_MODEL = \"Salesforce/blip-image-captioning-base\"   #ou \"Salesforce/blip-image-captioning-large\", mais pesado\n",
    "CAPTION_MAX_TOKENS = 160\n",
    "CAPTION_MAX_WORDS = 80\n",
    "\n",
    "FRAME_LIMIT = 1000 #Limitar número de frames processados (Bom para testes)\n",
    "\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c111508",
   "metadata": {},
   "source": [
    "Funções de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfd53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apenas para make sure diretório existe\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "#Para listar files com frames dentro de folder\n",
    "def list_frame_files(input_folder: str):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(input_folder):\n",
    "        for f in sorted(filenames):\n",
    "            if os.path.splitext(f)[1].lower() in IMAGE_EXTS:\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "#Capturar timestamp do filename frame\n",
    "def parse_timestamp_from_filename(fname: str):\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.search(r\"(\\d{4})[-_]?(\\d{2})[-_]?(\\d{2})[_T\\-]?(\\d{2})[:_]?(\\d{2})[:_]?(\\d{2})\", base)\n",
    "    if m:\n",
    "        year, mon, day, h, mn, s = m.groups()\n",
    "        try:\n",
    "            dt = datetime.datetime(int(year), int(mon), int(day), int(h), int(mn), int(s))\n",
    "            return dt.isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    m2 = re.search(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})\", base)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    ts = os.path.getmtime(fname)\n",
    "    return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "\n",
    "#Encurtar descrição baseado em MAX_WORDS\n",
    "def shorten_text(text: str, max_words: int = CAPTION_MAX_WORDS):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    first = re.split(r'[.?!]\\s*', text)[0].strip()\n",
    "    words = first.split()\n",
    "    if len(words) <= max_words:\n",
    "        return first\n",
    "    return \" \".join(words[:max_words])\n",
    "\n",
    "# Calcula o IoU (Intersection over Union) entre dois bounding boxes.\n",
    "# Retorna um valor entre 0 e 1 que indica a sobreposição das caixas.\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    if interArea == 0:\n",
    "        return 0.0\n",
    "    boxAArea = max(1.0, (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = max(1.0, (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30852dcd",
   "metadata": {},
   "source": [
    "Tracker baseado em IoU para dar ID único para indivíduos/grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60082698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTracker:\n",
    "    \"\"\"\n",
    "    Rastreador simples baseado em IoU (Intersection over Union).\n",
    "\n",
    "    A cada frame:\n",
    "      - Associa detecções novas a objetos existentes com base no IoU.\n",
    "      - Atribui IDs únicos a novos objetos.\n",
    "      - Remove objetos que sumiram há muitos frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iou_threshold=0.3, max_lost=30):\n",
    "        self.next_id = 1\n",
    "        self.tracks = {}\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_lost = max_lost\n",
    "\n",
    "    \"\"\"\n",
    "    Atualiza os rastros com as detecções do frame atual.\n",
    "    Retorna uma lista de tuplas (track_id, bounding_box).\n",
    "    \"\"\"\n",
    "    def update(self, detections: List[Tuple[float, float, float, float]], frame_idx: int):\n",
    "        assignments = []\n",
    "        unmatched_dets = set(range(len(detections)))\n",
    "        for tid, info in list(self.tracks.items()):\n",
    "            best_i = -1\n",
    "            best_iou = 0.0\n",
    "            for di in list(unmatched_dets):\n",
    "                val = iou(info['box'], detections[di])\n",
    "                if val > best_iou:\n",
    "                    best_iou = val\n",
    "                    best_i = di\n",
    "            if best_i != -1 and best_iou >= self.iou_threshold:\n",
    "                self.tracks[tid]['box'] = detections[best_i]\n",
    "                self.tracks[tid]['last_seen'] = frame_idx\n",
    "                self.tracks[tid]['lost'] = 0\n",
    "                assignments.append((tid, detections[best_i]))\n",
    "                unmatched_dets.remove(best_i)\n",
    "            else:\n",
    "                self.tracks[tid]['lost'] += 1\n",
    "        to_delete = [tid for tid, info in self.tracks.items() if info['lost'] > self.max_lost]\n",
    "        for tid in to_delete:\n",
    "            del self.tracks[tid]\n",
    "        for di in sorted(unmatched_dets):\n",
    "            tid = self.next_id\n",
    "            self.next_id += 1\n",
    "            self.tracks[tid] = {'box': detections[di], 'last_seen': frame_idx, 'lost': 0}\n",
    "            assignments.append((tid, detections[di]))\n",
    "        return assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4edc8f",
   "metadata": {},
   "source": [
    "Detector de indivíduos/grupos baseado em YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b189b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def __init__(self, weights: str = None, conf: float = 0.25, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        if weights:\n",
    "            self.model = YOLO(weights)\n",
    "        else:\n",
    "            self.model = YOLO(\"yolov8n\")\n",
    "        self.conf = conf\n",
    "\n",
    "    \"\"\"\n",
    "    Detecta todos os objetos que YOLO foi treinado para detectar e cria registros.\n",
    "    \"\"\"\n",
    "    def detect(self, image: np.ndarray):\n",
    "        res = self.model.predict(image, imgsz=640, conf=self.conf, verbose=False, device=self.device)\n",
    "        results = res[0]\n",
    "        detections = []\n",
    "        if getattr(results, 'boxes', None) is not None:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            scores = results.boxes.conf.cpu().numpy()\n",
    "            cls = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            names = getattr(self.model, \"names\", None)\n",
    "            for b, s, c in zip(boxes, scores, cls):\n",
    "                detections.append({\n",
    "                    'bbox': [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                    'confidence': float(s),\n",
    "                    'class_id': int(c),\n",
    "                    'class_name': names[int(c)] if names is not None and int(c) in names else str(int(c))\n",
    "                })\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec7526",
   "metadata": {},
   "source": [
    "BLIP é o módulo open-source para criar captions/descrições de acordo com imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124c68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPCaptioner:\n",
    "    def __init__(self, model_name=BLIP_MODEL, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    #Cria caption/descrição\n",
    "    def caption(self, pil_image: Image.Image, max_length=CAPTION_MAX_TOKENS):\n",
    "        inputs = self.processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(**inputs, max_new_tokens=max_length, num_beams=5, early_stopping=True)\n",
    "        text = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return shorten_text(text, max_words=CAPTION_MAX_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e107c1",
   "metadata": {},
   "source": [
    "CLIP é o módulo open-source (apesar de ser da openAI) responsável por de acordo com atributos/características atribuir à imagem segundo um fator de confiança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245d2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPAttributeMatcher:\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        self.hair_colors = [\"blond hair\", \"brown hair\", \"black hair\", \"gray hair\", \"red hair\", \"dark hair\", \"light hair\"]\n",
    "        self.clothing_items = [\"t-shirt\", \"shirt\", \"jacket\", \"hoodie\", \"dress\", \"coat\", \"suit\", \"skirt\", \"pants\", \"shorts\"]\n",
    "        self.clothing_colors = [\n",
    "            \"red shirt\", \"blue shirt\", \"black shirt\", \"white shirt\", \"green shirt\",\n",
    "            \"red jacket\", \"blue jacket\", \"black jacket\", \"white jacket\", \"green jacket\",\n",
    "            \"gray sweater\", \"striped shirt\", \"plaid shirt\"\n",
    "        ]\n",
    "        self.accessories = [\"wearing glasses\", \"wearing sunglasses\", \"wearing a hat\", \"backpack\", \"holding a bag\"]\n",
    "\n",
    "    #Dar score de confiança para os textos\n",
    "    def _score_texts(self, pil_image: Image.Image, texts: List[str]):\n",
    "        inputs = self.processor(text=texts, images=pil_image, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        with __import__('torch').no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=1).cpu().numpy().flatten()\n",
    "        return probs.tolist()\n",
    "\n",
    "    #Retornar os com maior confiança\n",
    "    def _top_filtered(self, pil_image: Image.Image, texts: List[str], top_k=2, min_conf=ATTR_CONF_THRESHOLD):\n",
    "        probs = self._score_texts(pil_image, texts)\n",
    "        idxs = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_k]\n",
    "        return [{\"label\": texts[i], \"score\": float(probs[i])} for i in idxs if probs[i] >= min_conf]\n",
    "\n",
    "    #Retornar os atributos com maior confiança para todas as categorias\n",
    "    def get_attributes(self, pil_image: Image.Image, top_k=2):\n",
    "        attrs = {}\n",
    "        attrs['hair'] = self._top_filtered(pil_image, self.hair_colors, top_k=top_k)\n",
    "        attrs['clothing_items'] = self._top_filtered(pil_image, self.clothing_items, top_k=top_k)\n",
    "        attrs['clothing_colors'] = self._top_filtered(pil_image, self.clothing_colors, top_k=top_k)\n",
    "        attrs['accessories'] = self._top_filtered(pil_image, self.accessories, top_k=top_k)\n",
    "        return attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db2a89",
   "metadata": {},
   "source": [
    "Pipeline principal unindo quase tudo, menos extração de frames de vídeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f09daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    ensure_dir(OUTPUT_CROPS_FOLDER)\n",
    "    ensure_dir(OUTPUT_RECORDS_FOLDER)\n",
    "\n",
    "    #Tentar utilizar GPU CUDA, se tiver\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    #Inicializando módulos\n",
    "    detector = Detector(weights=YOLO_WEIGHTS, conf=YOLO_CONF, device=device)\n",
    "    captioner = BLIPCaptioner(model_name=BLIP_MODEL, device=device)\n",
    "    attr_matcher = CLIPAttributeMatcher(device=device)\n",
    "    tracker = SimpleTracker(iou_threshold=IOU_MATCH_THRESHOLD, max_lost=30)\n",
    "\n",
    "    frame_files = list_frame_files(INPUT_FRAMES_FOLDER)\n",
    "    #Para limitar quantidade de frames processados (útil para testes rápidos)\n",
    "    if FRAME_LIMIT:\n",
    "        frame_files = frame_files[:FRAME_LIMIT]\n",
    "    if not frame_files:\n",
    "        print(\"Frames não foram encontrados em \", INPUT_FRAMES_FOLDER)\n",
    "        return\n",
    "\n",
    "    print(f\"Encontrou {len(frame_files)} frames, processando...\")\n",
    "\n",
    "\n",
    "\n",
    "    frame_idx = 0\n",
    "    track_records = {}\n",
    "    for frame_path in frame_files:\n",
    "        frame_idx += 1\n",
    "        pil = Image.open(frame_path).convert(\"RGB\")\n",
    "        img_np = np.array(pil)\n",
    "\n",
    "        dets = detector.detect(img_np)\n",
    "\n",
    "        #Utilizando apenas bounding boxes com pessoas\n",
    "        person_dets_xyxy = []\n",
    "        for d in dets:\n",
    "            name = d.get('class_name', '').lower()\n",
    "            if name in (\"person\", \"people\", \"human\") or int(d.get('class_id', -1)) == 0:\n",
    "                x1, y1, x2, y2 = [int(round(v)) for v in d['bbox']]\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                if area >= MIN_BOX_AREA:\n",
    "                    person_dets_xyxy.append([x1, y1, x2, y2])\n",
    "\n",
    "        assignments = tracker.update(person_dets_xyxy, frame_idx)\n",
    "        timestamp = parse_timestamp_from_filename(frame_path)\n",
    "        frame_base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "\n",
    "        for track_id, box in assignments:\n",
    "            x1, y1, x2, y2 = [int(v) for v in box]\n",
    "            crop = pil.crop((x1, y1, x2, y2))\n",
    "            #Dizem que BLIP trabalha melhor com 384 x 384, por isto o resize\n",
    "            crop_for_blip = crop.resize((384, 384), Image.LANCZOS)\n",
    "\n",
    "            if track_id not in track_records:\n",
    "                try:\n",
    "                    caption = captioner.caption(crop_for_blip)\n",
    "                except Exception as e:\n",
    "                    caption = \"\"\n",
    "                    print(\"Erro de geração de caption:\", e)\n",
    "                try:\n",
    "                    attributes = attr_matcher.get_attributes(crop_for_blip, top_k=ATTR_TOP_K)\n",
    "                except Exception as e:\n",
    "                    attributes = {}\n",
    "                    print(\"Erro de matching de atributo:\", e)\n",
    "\n",
    "                fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "                #Apenas salvando 1 record para cada indivíduo/grupo detectado\n",
    "                first_crop_fname = f\"cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}_first.jpg\"\n",
    "                first_crop_path = os.path.join(OUTPUT_CROPS_FOLDER, first_crop_fname)\n",
    "                try:\n",
    "                    crop.save(first_crop_path, format=\"JPEG\", quality=90)\n",
    "                except Exception as e:\n",
    "                    print(\"Falha ao salvar primeiro crop:\", e)\n",
    "\n",
    "                track_records[track_id] = {\n",
    "                    \"track_id\": track_id,\n",
    "                    \"timestamp_start\": timestamp,\n",
    "                    \"timestamp_end\": timestamp,\n",
    "                    \"first_crop_path\": first_crop_path,\n",
    "                    \"last_crop_path\": first_crop_path,\n",
    "                    \"description\": caption,\n",
    "                    \"attributes\": attributes,\n",
    "                    \"bbox_start\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                    \"bbox_end\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                }\n",
    "            else:\n",
    "                fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "                last_crop_fname = f\"cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}_last.jpg\"\n",
    "                last_crop_path = os.path.join(OUTPUT_CROPS_FOLDER, last_crop_fname)\n",
    "                try:\n",
    "                    crop.save(last_crop_path, format=\"JPEG\", quality=90)\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to save last crop:\", e)\n",
    "\n",
    "                track_records[track_id][\"timestamp_end\"] = timestamp\n",
    "                track_records[track_id][\"last_crop_path\"] = last_crop_path\n",
    "                track_records[track_id][\"bbox_end\"] = {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2}\n",
    "\n",
    "        #Simulando TQDM\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"Processou {frame_idx}/{len(frame_files)} frames\")\n",
    "\n",
    "    #Salvar records\n",
    "    for track_id, rec in track_records.items():\n",
    "        record_fname = f\"record_cam{CAMERA_ID}_trk{track_id}.json\"\n",
    "        record_path = os.path.join(OUTPUT_RECORDS_FOLDER, record_fname)\n",
    "        try:\n",
    "            with open(record_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                json.dump(rec, fh, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(\"Falhou escrita record:\", e)\n",
    "\n",
    "    print(\"Pronto. Crops pessoas salvo em:\", OUTPUT_CROPS_FOLDER)\n",
    "    print(\"Records salvo em:\", OUTPUT_RECORDS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf62d9a",
   "metadata": {},
   "source": [
    "Rodar main pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68328eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Encontrou 1 frames, processando...\n",
      "Pronto. Crops pessoas salvo em: CroppedPersonsHighRes\n",
      "Records salvo em: RecordsHighRes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\OneDrive - Unesp\\Documentos\\GitHub\\BD2-Trabalho\\torch-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rodri\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
