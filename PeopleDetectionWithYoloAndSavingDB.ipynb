{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7edee78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3baf663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gotoxico/torch-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import datetime\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Detector: ultralytics YOLO\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install 'ultralytics' (pip install ultralytics).\") from e\n",
    "\n",
    "# BLIP captioning from Hugging Face / transformers\n",
    "try:\n",
    "    from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install 'transformers' and 'torch' (pip install transformers torch).\") from e\n",
    "\n",
    "# DB\n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Text, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07266519",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aeaae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIG ==============\n",
    "# Input frames folder (each frame is an image file)\n",
    "INPUT_FRAMES_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Frames\"           # folder with subfolders or images\n",
    "OUTPUT_CROPS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/CroppedPersons\"           # where cropped person images will be saved\n",
    "OUTPUT_RECORDS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Records\"   # <-- new folder for JSON records\n",
    "\n",
    "CAMERA_ID = 1                              # camera identifier to store in DB\n",
    "YOLO_WEIGHTS = \"yolo11n.pt\"                        # None => use 'yolov8n' builtin; or path to your .pt (e.g., \"yolov11.pt\")\n",
    "YOLO_CONF = 0.35                           # detection confidence threshold\n",
    "IOU_MATCH_THRESHOLD = 0.3                  # tracker IoU threshold to match detections across frames\n",
    "MIN_BOX_AREA = 400                         # ignore tiny boxes\n",
    "# BLIP model (Hugging Face)\n",
    "BLIP_MODEL = \"Salesforce/blip-image-captioning-base\"   # or \"Salesforce/blip-image-captioning-large\"\n",
    "# PostgreSQL connection string (SQLAlchemy)\n",
    "password = quote_plus(\"Skatingpussy1989@\")\n",
    "DB_URL = \"postgresql+psycopg2://postgres:{password}@localhost:5432/BD2\"\n",
    "# Optional: limit number of frames processed (None for all)\n",
    "FRAME_LIMIT = None\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# Supported image extensions\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5939748",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcaae31",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    894\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 284\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone. Crops saved to:\u001b[39m\u001b[33m\"\u001b[39m, OUTPUT_CROPS_FOLDER)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 199\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# Prepare DB\u001b[39;00m\n\u001b[32m    198\u001b[39m engine = create_engine(DB_URL, echo=\u001b[38;5;28;01mFalse\u001b[39;00m, future=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m records_table = \u001b[43mcreate_metadata_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[32m    202\u001b[39m detector = Detector(weights=YOLO_WEIGHTS, conf=YOLO_CONF)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mcreate_metadata_table\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m    176\u001b[39m meta = MetaData()\n\u001b[32m    177\u001b[39m records = Table(\n\u001b[32m    178\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mperson_tracks\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    179\u001b[39m     meta,\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m     Column(\u001b[33m\"\u001b[39m\u001b[33mimage_path\u001b[39m\u001b[33m\"\u001b[39m, String, nullable=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    187\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[43mmeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/sql/schema.py:5928\u001b[39m, in \u001b[36mMetaData.create_all\u001b[39m\u001b[34m(self, bind, tables, checkfirst)\u001b[39m\n\u001b[32m   5904\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_all\u001b[39m(\n\u001b[32m   5905\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5906\u001b[39m     bind: _CreateDropBind,\n\u001b[32m   5907\u001b[39m     tables: Optional[_typing_Sequence[Table]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5908\u001b[39m     checkfirst: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   5909\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5910\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create all tables stored in this metadata.\u001b[39;00m\n\u001b[32m   5911\u001b[39m \n\u001b[32m   5912\u001b[39m \u001b[33;03m    Conditional by default, will not attempt to recreate tables already\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5926\u001b[39m \n\u001b[32m   5927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5928\u001b[39m     \u001b[43mbind\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mddl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSchemaGenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtables\u001b[49m\n\u001b[32m   5930\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:3251\u001b[39m, in \u001b[36mEngine._run_ddl_visitor\u001b[39m\u001b[34m(self, visitorcallable, element, **kwargs)\u001b[39m\n\u001b[32m   3245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_ddl_visitor\u001b[39m(\n\u001b[32m   3246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3247\u001b[39m     visitorcallable: Type[InvokeDDLBase],\n\u001b[32m   3248\u001b[39m     element: SchemaVisitable,\n\u001b[32m   3249\u001b[39m     **kwargs: Any,\n\u001b[32m   3250\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3251\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitorcallable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:3241\u001b[39m, in \u001b[36mEngine.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3216\u001b[39m \u001b[38;5;129m@contextlib\u001b[39m.contextmanager\n\u001b[32m   3217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbegin\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Connection]:\n\u001b[32m   3218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a context manager delivering a :class:`_engine.Connection`\u001b[39;00m\n\u001b[32m   3219\u001b[39m \u001b[33;03m    with a :class:`.Transaction` established.\u001b[39;00m\n\u001b[32m   3220\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3239\u001b[39m \n\u001b[32m   3240\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3241\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m   3242\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m conn.begin():\n\u001b[32m   3243\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m conn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:3277\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3255\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3256\u001b[39m \n\u001b[32m   3257\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3274\u001b[39m \n\u001b[32m   3275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:2440\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2439\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2440\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2442\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:143\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    145\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    146\u001b[39m             err, dialect, engine\n\u001b[32m    147\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/base.py:3301\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3281\u001b[39m \n\u001b[32m   3282\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3299\u001b[39m \n\u001b[32m   3300\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:447\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    440\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m \n\u001b[32m    446\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:711\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    709\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    714\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/impl.py:175\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:388\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    386\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:673\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:899\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/pool/base.py:895\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    897\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/create.py:661\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    658\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sqlalchemy/engine/default.py:629\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "pipeline_frames_to_db.py\n",
    "\n",
    "- Input: folder with frame images (any common image extension).\n",
    "- Detect people with a YOLO model (Ultralytics YOLO API used here; you can point to a YOLOv11 weights file).\n",
    "- Track across frames using a simple IoU-based tracker (stable IDs).\n",
    "- Crop each detection and caption it with BLIP (Hugging Face).\n",
    "- Save crop images and metadata to PostgreSQL.\n",
    "\n",
    "Configure paths and DB settings in the CONFIG section.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def ensure_dir(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def list_frame_files(input_folder: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(input_folder):\n",
    "        for f in sorted(filenames):\n",
    "            if os.path.splitext(f)[1].lower() in IMAGE_EXTS:\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "def parse_timestamp_from_filename(fname: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to parse timestamp from filename using common patterns:\n",
    "    - 20250915_143321\n",
    "    - 2025-09-15T14:33:21\n",
    "    - frame_000123 (fallback to file mtime)\n",
    "    Returns ISO 8601 string.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    # Patterns\n",
    "    m = re.search(r\"(\\d{4})[-_]?(\\d{2})[-_]?(\\d{2})[_T\\-]?(\\d{2})[:_]?(\\d{2})[:_]?(\\d{2})\", base)\n",
    "    if m:\n",
    "        year,mon,day,h,mn,s = m.groups()\n",
    "        try:\n",
    "            dt = datetime.datetime(int(year),int(mon),int(day),int(h),int(mn),int(s))\n",
    "            return dt.isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # ISO-like\n",
    "    m2 = re.search(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})\", base)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    # fallback: file modification time\n",
    "    ts = os.path.getmtime(fname)\n",
    "    return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "\n",
    "# ---------- Simple IoU tracker (replaceable by ByteTrack) ----------\n",
    "def iou(boxA, boxB):\n",
    "    # boxes are [x1,y1,x2,y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    if interArea == 0:\n",
    "        return 0.0\n",
    "    boxAArea = (boxA[2]-boxA[0])*(boxA[3]-boxA[1])\n",
    "    boxBArea = (boxB[2]-boxB[0])*(boxB[3]-boxB[1])\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "class SimpleTracker:\n",
    "    \"\"\"\n",
    "    Very small IoU-based tracker that assigns stable IDs.\n",
    "    Keeps last box per track and matches by IoU threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, iou_threshold=0.3, max_lost=30):\n",
    "        self.next_id = 1\n",
    "        self.tracks = {}  # id -> {box, last_seen_frame_index, lost}\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_lost = max_lost\n",
    "\n",
    "    def update(self, detections: List[Tuple[float, float, float, float]], frame_idx:int):\n",
    "        \"\"\"\n",
    "        detections: list of boxes [x1,y1,x2,y2]\n",
    "        returns list of (track_id, box)\n",
    "        \"\"\"\n",
    "        assignments = []\n",
    "        unmatched_dets = set(range(len(detections)))\n",
    "        # compute matches\n",
    "        for tid, info in list(self.tracks.items()):\n",
    "            best_i = -1\n",
    "            best_iou = 0.0\n",
    "            for di in unmatched_dets:\n",
    "                val = iou(info['box'], detections[di])\n",
    "                if val > best_iou:\n",
    "                    best_iou = val\n",
    "                    best_i = di\n",
    "            if best_i != -1 and best_iou >= self.iou_threshold:\n",
    "                # match\n",
    "                self.tracks[tid]['box'] = detections[best_i]\n",
    "                self.tracks[tid]['last_seen'] = frame_idx\n",
    "                self.tracks[tid]['lost'] = 0\n",
    "                assignments.append((tid, detections[best_i]))\n",
    "                unmatched_dets.remove(best_i)\n",
    "            else:\n",
    "                # increment lost\n",
    "                self.tracks[tid]['lost'] += 1\n",
    "\n",
    "        # remove dead tracks\n",
    "        to_delete = [tid for tid, info in self.tracks.items() if info['lost'] > self.max_lost]\n",
    "        for tid in to_delete:\n",
    "            del self.tracks[tid]\n",
    "\n",
    "        # create new tracks for unmatched detections\n",
    "        for di in sorted(unmatched_dets):\n",
    "            tid = self.next_id\n",
    "            self.next_id += 1\n",
    "            self.tracks[tid] = {'box': detections[di], 'last_seen': frame_idx, 'lost': 0}\n",
    "            assignments.append((tid, detections[di]))\n",
    "\n",
    "        return assignments\n",
    "\n",
    "# ---------- Detector wrapper ----------\n",
    "class Detector:\n",
    "    def __init__(self, weights: str = None, conf: float = 0.25):\n",
    "        # If weights is None, use a small built-in YOLO model (yolov8n)\n",
    "        if weights:\n",
    "            self.model = YOLO(weights)\n",
    "        else:\n",
    "            # ultralytics accepts 'yolov8n.pt' string or model name\n",
    "            self.model = YOLO(\"yolov8n\")  # change if you want another\n",
    "        self.conf = conf\n",
    "\n",
    "    def detect(self, image: np.ndarray):\n",
    "        \"\"\"\n",
    "        image: HxWxC BGR (OpenCV style) or RGB numpy\n",
    "        returns: list of detections as dicts: {bbox: [x1,y1,x2,y2], confidence, class_id, class_name}\n",
    "        \"\"\"\n",
    "        # ultralytics expects either path or numpy (RGB)\n",
    "        # convert BGR->RGB if needed: assume input is RGB\n",
    "        res = self.model.predict(image, imgsz=640, conf=self.conf, verbose=False)\n",
    "        # res is a list (batch); get first\n",
    "        results = res[0]\n",
    "        detections = []\n",
    "        if results.boxes is not None:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()  # Nx4\n",
    "            scores = results.boxes.conf.cpu().numpy()\n",
    "            cls = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            for b,s,c in zip(boxes, scores, cls):\n",
    "                detections.append({\n",
    "                    'bbox':[float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                    'confidence':float(s),\n",
    "                    'class_id':int(c),\n",
    "                    'class_name': self.model.names[c] if hasattr(self.model, \"names\") else str(c)\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "# ---------- Captioner (BLIP) ----------\n",
    "class BLIPCaptioner:\n",
    "    def __init__(self, model_name=BLIP_MODEL, device=\"cuda\"):\n",
    "        import torch\n",
    "        self.device = device\n",
    "        print(\"Loading BLIP model (this may take a while)...\")\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def caption(self, pil_image: Image.Image, max_length=30) -> str:\n",
    "        import torch\n",
    "        inputs = self.processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(**inputs, max_new_tokens=max_length)\n",
    "        text = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "# ---------- Database (SQLAlchemy) ----------\n",
    "def create_metadata_table(engine):\n",
    "    meta = MetaData()\n",
    "    records = Table(\n",
    "        \"person_tracks\",\n",
    "        meta,\n",
    "        Column(\"id\", Integer, primary_key=True, autoincrement=True),\n",
    "        Column(\"track_id\", Integer, nullable=False),\n",
    "        Column(\"timestamp\", TIMESTAMP, nullable=False),\n",
    "        Column(\"camera_id\", Integer, nullable=True),\n",
    "        Column(\"description\", Text, nullable=True),\n",
    "        Column(\"bbox\", JSONB, nullable=False),   # store as json object {x1,y1,x2,y2}\n",
    "        Column(\"image_path\", String, nullable=False)\n",
    "    )\n",
    "    meta.create_all(engine)\n",
    "    return records\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "def run_pipeline():\n",
    "    # Prepare folders\n",
    "    ensure_dir = lambda p: os.makedirs(p, exist_ok=True) if not os.path.exists(p) else None\n",
    "    ensure_dir(OUTPUT_CROPS_FOLDER)\n",
    "\n",
    "    # Prepare DB\n",
    "    engine = create_engine(DB_URL, echo=False, future=True)\n",
    "    records_table = create_metadata_table(engine)\n",
    "\n",
    "    # Load models\n",
    "    detector = Detector(weights=YOLO_WEIGHTS, conf=YOLO_CONF)\n",
    "    device = \"cuda\" if (os.environ.get(\"CUDA_VISIBLE_DEVICES\") is not None or os.name != \"nt\") else \"cpu\"\n",
    "    # If torch.cuda.is_available() is desired use try/except import torch\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        device = \"cpu\"\n",
    "    captioner = BLIPCaptioner(model_name=BLIP_MODEL, device=device)\n",
    "\n",
    "    # Tracker (simple)\n",
    "    tracker = SimpleTracker(iou_threshold=IOU_MATCH_THRESHOLD, max_lost=30)\n",
    "\n",
    "    frame_files = list_frame_files(INPUT_FRAMES_FOLDER)\n",
    "    if FRAME_LIMIT:\n",
    "        frame_files = frame_files[:FRAME_LIMIT]\n",
    "    if not frame_files:\n",
    "        print(\"No frames found in\", INPUT_FRAMES_FOLDER)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(frame_files)} frames, processing...\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    with engine.begin() as conn:\n",
    "        for frame_path in frame_files:\n",
    "            frame_idx += 1\n",
    "            # Load image (as RGB numpy for ultralytics + PIL for crop)\n",
    "            pil = Image.open(frame_path).convert(\"RGB\")\n",
    "            img_np = np.array(pil)  # RGB\n",
    "            # Run detector\n",
    "            dets = detector.detect(img_np)\n",
    "            # Filter only 'person' class if model uses coco names\n",
    "            person_dets = []\n",
    "            for d in dets:\n",
    "                name = d.get('class_name', '').lower()\n",
    "                if name in (\"person\", \"people\", \"human\") or int(d.get('class_id', -1)) == 0:\n",
    "                    x1,y1,x2,y2 = [int(round(v)) for v in d['bbox']]\n",
    "                    area = (x2-x1)*(y2-y1)\n",
    "                    if area >= MIN_BOX_AREA:\n",
    "                        person_dets.append([x1,y1,x2,y2])\n",
    "\n",
    "            # Update tracker and get assignments\n",
    "            assignments = tracker.update(person_dets, frame_idx)  # list of (track_id, box)\n",
    "            timestamp = parse_timestamp_from_filename(frame_path)\n",
    "\n",
    "            for track_id, box in assignments:\n",
    "                x1,y1,x2,y2 = [int(v) for v in box]\n",
    "                # crop (use PIL)\n",
    "                crop = pil.crop((x1, y1, x2, y2))\n",
    "                # optional: resize crop to reasonable size for BLIP (e.g., 384x384)\n",
    "                crop_for_blip = crop.resize((384,384), Image.LANCZOS)\n",
    "                # caption\n",
    "                try:\n",
    "                    caption = captioner.caption(crop_for_blip)\n",
    "                except Exception as e:\n",
    "                    caption = \"\"\n",
    "                    print(\"Caption error:\", e)\n",
    "\n",
    "                # save crop image with filename including track and timestamp\n",
    "                fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "                fname = f\"cam{CAMERA_ID}_trk{track_id}_{os.path.splitext(os.path.basename(frame_path))[0]}_{fname_ts}.jpg\"\n",
    "                out_path = os.path.join(OUTPUT_CROPS_FOLDER, fname)\n",
    "                crop.save(out_path, format=\"JPEG\", quality=90)\n",
    "\n",
    "                # insert record into DB\n",
    "                rec = {\n",
    "                    \"track_id\": int(track_id),\n",
    "                    \"timestamp\": datetime.datetime.fromisoformat(timestamp),\n",
    "                    \"camera_id\": int(CAMERA_ID),\n",
    "                    \"description\": caption,\n",
    "                    \"bbox\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                    \"image_path\": out_path\n",
    "                }\n",
    "                # insert\n",
    "                conn.execute(records_table.insert().values(**rec))\n",
    "\n",
    "            if frame_idx % 50 == 0:\n",
    "                print(f\"Processed {frame_idx}/{len(frame_files)} frames\")\n",
    "\n",
    "    print(\"Done. Crops saved to:\", OUTPUT_CROPS_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b1472",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09a0317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading BLIP model (this may take a while)...\n",
      "Found 78347 frames, processing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 246\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRecords saved to:\u001b[39m\u001b[33m\"\u001b[39m, OUTPUT_RECORDS_FOLDER)\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    206\u001b[39m crop_for_blip = crop.resize((\u001b[32m384\u001b[39m, \u001b[32m384\u001b[39m), Image.LANCZOS)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     caption = \u001b[43mcaptioner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_for_blip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    210\u001b[39m     caption = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mBLIPCaptioner.caption\u001b[39m\u001b[34m(self, pil_image, max_length)\u001b[39m\n\u001b[32m    145\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.processor(images=pil_image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m text = \u001b[38;5;28mself\u001b[39m.processor.decode(out[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip.py:949\u001b[39m, in \u001b[36mBlipForConditionalGeneration.generate\u001b[39m\u001b[34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[39m\n\u001b[32m    946\u001b[39m input_ids[:, \u001b[32m0\u001b[39m] = \u001b[38;5;28mself\u001b[39m.config.text_config.bos_token_id\n\u001b[32m    947\u001b[39m attention_mask = attention_mask[:, :-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43msep_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:908\u001b[39m, in \u001b[36mBlipTextLMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction, cache_position)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m     use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    926\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:809\u001b[39m, in \u001b[36mBlipTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder, cache_position)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    807\u001b[39m     embedding_output = encoder_embeds\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    823\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:460\u001b[39m, in \u001b[36mBlipTextEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    456\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    458\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:369\u001b[39m, in \u001b[36mBlipTextLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    368\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    378\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:297\u001b[39m, in \u001b[36mBlipTextAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    296\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    307\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/transformers/models/blip/modeling_blip_text.py:202\u001b[39m, in \u001b[36mBlipTextSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    199\u001b[39m             past_key_values.is_updated[\u001b[38;5;28mself\u001b[39m.layer_idx] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m attention_scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33mrelative_key\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33mrelative_key_query\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    205\u001b[39m     seq_length = hidden_states.size()[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "pipeline_frames_to_files.py\n",
    "\n",
    "Same pipeline as before, but instead of saving records to PostgreSQL, it writes each\n",
    "record as a JSON file into OUTPUT_RECORDS_FOLDER. Cropped person images are saved to\n",
    "OUTPUT_CROPS_FOLDER as before.\n",
    "\n",
    "Configure paths and options in the CONFIG section.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def ensure_dir(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def list_frame_files(input_folder: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(input_folder):\n",
    "        for f in sorted(filenames):\n",
    "            if os.path.splitext(f)[1].lower() in IMAGE_EXTS:\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "\n",
    "def parse_timestamp_from_filename(fname: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to parse timestamp from filename using common patterns:\n",
    "    - 20250915_143321\n",
    "    - 2025-09-15T14:33:21\n",
    "    - frame_000123 (fallback to file mtime)\n",
    "    Returns ISO 8601 string.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.search(r\"(\\d{4})[-_]?(\\d{2})[-_]?(\\d{2})[_T\\-]?(\\d{2})[:_]?(\\d{2})[:_]?(\\d{2})\", base)\n",
    "    if m:\n",
    "        year, mon, day, h, mn, s = m.groups()\n",
    "        try:\n",
    "            dt = datetime.datetime(int(year), int(mon), int(day), int(h), int(mn), int(s))\n",
    "            return dt.isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    m2 = re.search(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})\", base)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    ts = os.path.getmtime(fname)\n",
    "    return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "\n",
    "\n",
    "# ---------- Simple IoU tracker ----------\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    if interArea == 0:\n",
    "        return 0.0\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "\n",
    "class SimpleTracker:\n",
    "    def __init__(self, iou_threshold=0.3, max_lost=30):\n",
    "        self.next_id = 1\n",
    "        self.tracks = {}  # id -> {box, last_seen_frame_index, lost}\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_lost = max_lost\n",
    "\n",
    "    def update(self, detections: List[Tuple[float, float, float, float]], frame_idx: int):\n",
    "        assignments = []\n",
    "        unmatched_dets = set(range(len(detections)))\n",
    "        for tid, info in list(self.tracks.items()):\n",
    "            best_i = -1\n",
    "            best_iou = 0.0\n",
    "            for di in unmatched_dets:\n",
    "                val = iou(info['box'], detections[di])\n",
    "                if val > best_iou:\n",
    "                    best_iou = val\n",
    "                    best_i = di\n",
    "            if best_i != -1 and best_iou >= self.iou_threshold:\n",
    "                self.tracks[tid]['box'] = detections[best_i]\n",
    "                self.tracks[tid]['last_seen'] = frame_idx\n",
    "                self.tracks[tid]['lost'] = 0\n",
    "                assignments.append((tid, detections[best_i]))\n",
    "                unmatched_dets.remove(best_i)\n",
    "            else:\n",
    "                self.tracks[tid]['lost'] += 1\n",
    "        to_delete = [tid for tid, info in self.tracks.items() if info['lost'] > self.max_lost]\n",
    "        for tid in to_delete:\n",
    "            del self.tracks[tid]\n",
    "        for di in sorted(unmatched_dets):\n",
    "            tid = self.next_id\n",
    "            self.next_id += 1\n",
    "            self.tracks[tid] = {'box': detections[di], 'last_seen': frame_idx, 'lost': 0}\n",
    "            assignments.append((tid, detections[di]))\n",
    "        return assignments\n",
    "\n",
    "\n",
    "# ---------- Detector wrapper ----------\n",
    "class Detector:\n",
    "    def __init__(self, weights: str = None, conf: float = 0.25, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        if weights:\n",
    "            self.model = YOLO(weights)\n",
    "        else:\n",
    "            self.model = YOLO(\"yolov8n\")\n",
    "        self.conf = conf\n",
    "\n",
    "    def detect(self, image: np.ndarray):\n",
    "        # ultralytics expects numpy RGB\n",
    "        res = self.model.predict(image, imgsz=640, conf=self.conf, verbose=False, device=self.device)\n",
    "        results = res[0]\n",
    "        detections = []\n",
    "        if results.boxes is not None:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            scores = results.boxes.conf.cpu().numpy()\n",
    "            cls = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            for b, s, c in zip(boxes, scores, cls):\n",
    "                detections.append({\n",
    "                    'bbox': [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                    'confidence': float(s),\n",
    "                    'class_id': int(c),\n",
    "                    'class_name': self.model.names[c] if hasattr(self.model, \"names\") else str(c)\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "\n",
    "# ---------- Captioner (BLIP) ----------\n",
    "class BLIPCaptioner:\n",
    "    def __init__(self, model_name=BLIP_MODEL, device=\"cpu\"):\n",
    "        import torch\n",
    "        self.device = device\n",
    "        print(\"Loading BLIP model (this may take a while)...\")\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def caption(self, pil_image: Image.Image, max_length=60) -> str:\n",
    "        import torch\n",
    "        inputs = self.processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(**inputs, max_new_tokens=max_length)\n",
    "        text = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "def run_pipeline():\n",
    "    # Prepare folders\n",
    "    ensure_dir(OUTPUT_CROPS_FOLDER)\n",
    "    ensure_dir(OUTPUT_RECORDS_FOLDER)\n",
    "\n",
    "    # Load models\n",
    "    # device detection\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    detector = Detector(weights=YOLO_WEIGHTS, conf=YOLO_CONF, device=device)\n",
    "    captioner = BLIPCaptioner(model_name=BLIP_MODEL, device=device)\n",
    "\n",
    "    # Tracker\n",
    "    tracker = SimpleTracker(iou_threshold=IOU_MATCH_THRESHOLD, max_lost=30)\n",
    "\n",
    "    frame_files = list_frame_files(INPUT_FRAMES_FOLDER)\n",
    "    if FRAME_LIMIT:\n",
    "        frame_files = frame_files[:FRAME_LIMIT]\n",
    "    if not frame_files:\n",
    "        print(\"No frames found in\", INPUT_FRAMES_FOLDER)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(frame_files)} frames, processing...\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    for frame_path in frame_files:\n",
    "        frame_idx += 1\n",
    "        pil = Image.open(frame_path).convert(\"RGB\")\n",
    "        img_np = np.array(pil)  # RGB\n",
    "        dets = detector.detect(img_np)\n",
    "\n",
    "        # keep only person detections\n",
    "        person_dets_xyxy = []\n",
    "        for d in dets:\n",
    "            name = d.get('class_name', '').lower()\n",
    "            if name in (\"person\", \"people\", \"human\") or int(d.get('class_id', -1)) == 0:\n",
    "                x1, y1, x2, y2 = [int(round(v)) for v in d['bbox']]\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                if area >= MIN_BOX_AREA:\n",
    "                    person_dets_xyxy.append([x1, y1, x2, y2])\n",
    "\n",
    "        assignments = tracker.update(person_dets_xyxy, frame_idx)\n",
    "        timestamp = parse_timestamp_from_filename(frame_path)\n",
    "        frame_base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "\n",
    "        for track_id, box in assignments:\n",
    "            x1, y1, x2, y2 = [int(v) for v in box]\n",
    "            crop = pil.crop((x1, y1, x2, y2))\n",
    "            crop_for_blip = crop.resize((384, 384), Image.LANCZOS)\n",
    "            try:\n",
    "                caption = captioner.caption(crop_for_blip)\n",
    "            except Exception as e:\n",
    "                caption = \"\"\n",
    "                print(\"Caption error:\", e)\n",
    "\n",
    "            # save crop image\n",
    "            fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "            img_fname = f\"cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}.jpg\"\n",
    "            out_img_path = os.path.join(OUTPUT_CROPS_FOLDER, img_fname)\n",
    "            crop.save(out_img_path, format=\"JPEG\", quality=90)\n",
    "\n",
    "            # build record dict\n",
    "            record = {\n",
    "                \"track_id\": int(track_id),\n",
    "                \"timestamp\": timestamp,\n",
    "                \"camera_id\": int(CAMERA_ID),\n",
    "                \"description\": caption,\n",
    "                \"bbox\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                \"image_path\": out_img_path\n",
    "            }\n",
    "\n",
    "            # save record JSON with a unique filename\n",
    "            record_fname = f\"record_cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}.json\"\n",
    "            record_path = os.path.join(OUTPUT_RECORDS_FOLDER, record_fname)\n",
    "            try:\n",
    "                with open(record_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                    json.dump(record, fh, ensure_ascii=False, indent=2)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to write record:\", e)\n",
    "\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"Processed {frame_idx}/{len(frame_files)} frames\")\n",
    "\n",
    "    print(\"Done. Crops saved to:\", OUTPUT_CROPS_FOLDER)\n",
    "    print(\"Records saved to:\", OUTPUT_RECORDS_FOLDER)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd230c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5c196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gotoxico/torch-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CLIP model for attributes...\n",
      "Found 1000 frames, processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/1000 frames\n",
      "Processed 100/1000 frames\n",
      "Processed 150/1000 frames\n",
      "Processed 200/1000 frames\n",
      "Processed 250/1000 frames\n",
      "Processed 300/1000 frames\n",
      "Processed 350/1000 frames\n",
      "Processed 400/1000 frames\n",
      "Processed 450/1000 frames\n",
      "Processed 500/1000 frames\n",
      "Processed 550/1000 frames\n",
      "Processed 600/1000 frames\n",
      "Processed 650/1000 frames\n",
      "Processed 700/1000 frames\n",
      "Processed 750/1000 frames\n",
      "Processed 800/1000 frames\n",
      "Processed 850/1000 frames\n",
      "Processed 900/1000 frames\n",
      "Processed 950/1000 frames\n",
      "Processed 1000/1000 frames\n",
      "Done. Crops saved to: /home/gotoxico/BD2-Trabalho/projetoBD2/CroppedPersons\n",
      "Records saved to: /home/gotoxico/BD2-Trabalho/projetoBD2/Records\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "pipeline_frames_to_files_with_attributes.py\n",
    "\n",
    "Full pipeline:\n",
    "- Uses Ultralytics YOLO (v8) for person detection\n",
    "- Uses BLIP for short captions\n",
    "- Uses CLIP zero-shot to extract non-sensitive attributes (hair color, clothing item, clothing color, accessories)\n",
    "- Saves cropped person images and JSON records to disk\n",
    "\n",
    "Important: this code intentionally does NOT infer or record sensitive attributes such as skin tone, race, religion, or sexual orientation.\n",
    "\n",
    "Requirements:\n",
    "  pip install ultralytics transformers torch pillow numpy opencv-python\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# -------- CONFIG --------\n",
    "INPUT_FRAMES_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Frames\"\n",
    "OUTPUT_CROPS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/CroppedPersons\"\n",
    "OUTPUT_RECORDS_FOLDER = \"/home/gotoxico/BD2-Trabalho/projetoBD2/Records\"\n",
    "\n",
    "YOLO_WEIGHTS = \"yolo11n.pt\"       \n",
    "YOLO_CONF = 0.35    \n",
    "BLIP_MODEL = \"Salesforce/blip-image-captioning-base\"\n",
    "CAPTION_MAX_TOKENS = 160\n",
    "CAPTION_MAX_WORDS = 80\n",
    "\n",
    "FRAME_LIMIT = 1000\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "CAMERA_ID = 1\n",
    "MIN_BOX_AREA = 900\n",
    "IOU_MATCH_THRESHOLD = 0.3\n",
    "ATTR_TOP_K = 2\n",
    "ATTR_CONF_THRESHOLD = 0.05\n",
    "\n",
    "# -------- optional imports (fail gracefully) ----------\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except Exception:\n",
    "    YOLO = None\n",
    "\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "except Exception:\n",
    "    BlipProcessor = None\n",
    "    BlipForConditionalGeneration = None\n",
    "\n",
    "try:\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "except Exception:\n",
    "    CLIPProcessor = None\n",
    "    CLIPModel = None\n",
    "\n",
    "# -------- Utilities --------\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def list_frame_files(input_folder: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(input_folder):\n",
    "        for f in sorted(filenames):\n",
    "            if os.path.splitext(f)[1].lower() in IMAGE_EXTS:\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "def parse_timestamp_from_filename(fname: str) -> str:\n",
    "    base = os.path.basename(fname)\n",
    "    m = re.search(r\"(\\d{4})[-_]?(\\d{2})[-_]?(\\d{2})[_T\\-]?(\\d{2})[:_]?(\\d{2})[:_]?(\\d{2})\", base)\n",
    "    if m:\n",
    "        year, mon, day, h, mn, s = m.groups()\n",
    "        try:\n",
    "            dt = datetime.datetime(int(year), int(mon), int(day), int(h), int(mn), int(s))\n",
    "            return dt.isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    m2 = re.search(r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})\", base)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    ts = os.path.getmtime(fname)\n",
    "    return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "\n",
    "def shorten_text(text: str, max_words: int = CAPTION_MAX_WORDS) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    first = re.split(r'[.?!]\\s*', text)[0].strip()\n",
    "    words = first.split()\n",
    "    if len(words) <= max_words:\n",
    "        return first\n",
    "    return \" \".join(words[:max_words])\n",
    "\n",
    "# --------- Simple IoU tracker ----------\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    if interArea == 0:\n",
    "        return 0.0\n",
    "    boxAArea = max(1.0, (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = max(1.0, (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "class SimpleTracker:\n",
    "    def __init__(self, iou_threshold=0.3, max_lost=30):\n",
    "        self.next_id = 1\n",
    "        self.tracks = {}\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_lost = max_lost\n",
    "\n",
    "    def update(self, detections: List[Tuple[float, float, float, float]], frame_idx: int):\n",
    "        assignments = []\n",
    "        unmatched_dets = set(range(len(detections)))\n",
    "        for tid, info in list(self.tracks.items()):\n",
    "            best_i = -1\n",
    "            best_iou = 0.0\n",
    "            for di in list(unmatched_dets):\n",
    "                val = iou(info['box'], detections[di])\n",
    "                if val > best_iou:\n",
    "                    best_iou = val\n",
    "                    best_i = di\n",
    "            if best_i != -1 and best_iou >= self.iou_threshold:\n",
    "                self.tracks[tid]['box'] = detections[best_i]\n",
    "                self.tracks[tid]['last_seen'] = frame_idx\n",
    "                self.tracks[tid]['lost'] = 0\n",
    "                assignments.append((tid, detections[best_i]))\n",
    "                unmatched_dets.remove(best_i)\n",
    "            else:\n",
    "                self.tracks[tid]['lost'] += 1\n",
    "        to_delete = [tid for tid, info in self.tracks.items() if info['lost'] > self.max_lost]\n",
    "        for tid in to_delete:\n",
    "            del self.tracks[tid]\n",
    "        for di in sorted(unmatched_dets):\n",
    "            tid = self.next_id\n",
    "            self.next_id += 1\n",
    "            self.tracks[tid] = {'box': detections[di], 'last_seen': frame_idx, 'lost': 0}\n",
    "            assignments.append((tid, detections[di]))\n",
    "        return assignments\n",
    "\n",
    "# --------- Detector wrapper ----------\n",
    "class Detector:\n",
    "    def __init__(self, weights: str = None, conf: float = 0.25, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        if YOLO is None:\n",
    "            raise RuntimeError(\"ultralytics YOLO not installed (pip install ultralytics)\")\n",
    "        if weights:\n",
    "            self.model = YOLO(weights)\n",
    "        else:\n",
    "            self.model = YOLO(\"yolov8n\")\n",
    "        self.conf = conf\n",
    "\n",
    "    def detect(self, image: np.ndarray):\n",
    "        res = self.model.predict(image, imgsz=640, conf=self.conf, verbose=False, device=self.device)\n",
    "        results = res[0]\n",
    "        detections = []\n",
    "        if getattr(results, 'boxes', None) is not None:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            scores = results.boxes.conf.cpu().numpy()\n",
    "            cls = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            names = getattr(self.model, \"names\", None)\n",
    "            for b, s, c in zip(boxes, scores, cls):\n",
    "                detections.append({\n",
    "                    'bbox': [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                    'confidence': float(s),\n",
    "                    'class_id': int(c),\n",
    "                    'class_name': names[int(c)] if names is not None and int(c) in names else str(int(c))\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "# --------- BLIP captioner ----------\n",
    "class BLIPCaptioner:\n",
    "    def __init__(self, model_name=BLIP_MODEL, device=\"cpu\"):\n",
    "        if BlipProcessor is None or BlipForConditionalGeneration is None:\n",
    "            raise RuntimeError(\"transformers with BLIP not installed (pip install transformers)\")\n",
    "        import torch\n",
    "        self.device = device\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def caption(self, pil_image: Image.Image, max_length=CAPTION_MAX_TOKENS) -> str:\n",
    "        import torch\n",
    "        inputs = self.processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(**inputs, max_new_tokens=max_length, num_beams=5, temperature=0.7, early_stopping=True)\n",
    "        text = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return shorten_text(text, max_words=CAPTION_MAX_WORDS)\n",
    "\n",
    "# --------- CLIP attribute matcher (zero-shot, non-sensitive attrs) ----------\n",
    "class CLIPAttributeMatcher:\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        if CLIPProcessor is None or CLIPModel is None:\n",
    "            raise RuntimeError(\"transformers with CLIP not installed (pip install transformers)\")\n",
    "        import torch\n",
    "        self.device = device\n",
    "        print(\"Loading CLIP model for attributes...\")\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        self.hair_colors = [\"blond hair\", \"brown hair\", \"black hair\", \"gray hair\", \"red hair\", \"dark hair\", \"light hair\"]\n",
    "        self.clothing_items = [\"t-shirt\", \"shirt\", \"jacket\", \"hoodie\", \"dress\", \"coat\", \"suit\", \"skirt\", \"pants\", \"shorts\"]\n",
    "        self.clothing_colors = [\n",
    "            \"red shirt\", \"blue shirt\", \"black shirt\", \"white shirt\", \"green shirt\",\n",
    "            \"red jacket\", \"blue jacket\", \"black jacket\", \"white jacket\", \"green jacket\",\n",
    "            \"gray sweater\", \"striped shirt\", \"plaid shirt\"\n",
    "        ]\n",
    "        self.accessories = [\"wearing glasses\", \"wearing sunglasses\", \"wearing a hat\", \"backpack\", \"holding a bag\"]\n",
    "\n",
    "    def _score_texts(self, pil_image: Image.Image, texts: List[str]):\n",
    "        inputs = self.processor(text=texts, images=pil_image, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        with __import__('torch').no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=1).cpu().numpy().flatten()\n",
    "        return probs.tolist()\n",
    "\n",
    "    def _top_filtered(self, pil_image: Image.Image, texts: List[str], top_k=2, min_conf=ATTR_CONF_THRESHOLD):\n",
    "        probs = self._score_texts(pil_image, texts)\n",
    "        idxs = sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:top_k]\n",
    "        return [{\"label\": texts[i], \"score\": float(probs[i])} for i in idxs if probs[i] >= min_conf]\n",
    "\n",
    "    def get_attributes(self, pil_image: Image.Image, top_k=2):\n",
    "        attrs = {}\n",
    "        attrs['hair'] = self._top_filtered(pil_image, self.hair_colors, top_k=top_k)\n",
    "        attrs['clothing_items'] = self._top_filtered(pil_image, self.clothing_items, top_k=top_k)\n",
    "        attrs['clothing_colors'] = self._top_filtered(pil_image, self.clothing_colors, top_k=top_k)\n",
    "        attrs['accessories'] = self._top_filtered(pil_image, self.accessories, top_k=top_k)\n",
    "        return attrs\n",
    "\n",
    "# --------- Main pipeline ----------\n",
    "def run_pipeline_single_record():\n",
    "    ensure_dir(OUTPUT_CROPS_FOLDER)\n",
    "    ensure_dir(OUTPUT_RECORDS_FOLDER)\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize detector, captioner, attr matcher, tracker\n",
    "    detector = Detector(weights=YOLO_WEIGHTS, conf=YOLO_CONF, device=device)\n",
    "    captioner = BLIPCaptioner(model_name=BLIP_MODEL, device=device)\n",
    "    attr_matcher = CLIPAttributeMatcher(device=device)\n",
    "    tracker = SimpleTracker(iou_threshold=IOU_MATCH_THRESHOLD, max_lost=30)\n",
    "\n",
    "    frame_files = list_frame_files(INPUT_FRAMES_FOLDER)\n",
    "    if FRAME_LIMIT:\n",
    "        frame_files = frame_files[:FRAME_LIMIT]\n",
    "    if not frame_files:\n",
    "        print(\"No frames found in\", INPUT_FRAMES_FOLDER)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(frame_files)} frames, processing...\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    track_records = {}\n",
    "\n",
    "    for frame_path in frame_files:\n",
    "        frame_idx += 1\n",
    "        pil = Image.open(frame_path).convert(\"RGB\")\n",
    "        img_np = np.array(pil)\n",
    "\n",
    "        # --- Detection ---\n",
    "        dets = detector.detect(img_np)\n",
    "\n",
    "        person_dets_xyxy = []\n",
    "        for d in dets:\n",
    "            name = d.get('class_name', '').lower()\n",
    "            if name in (\"person\", \"people\", \"human\") or int(d.get('class_id', -1)) == 0:\n",
    "                x1, y1, x2, y2 = [int(round(v)) for v in d['bbox']]\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                if area >= MIN_BOX_AREA:\n",
    "                    person_dets_xyxy.append([x1, y1, x2, y2])\n",
    "\n",
    "        assignments = tracker.update(person_dets_xyxy, frame_idx)\n",
    "        timestamp = parse_timestamp_from_filename(frame_path)\n",
    "        frame_base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "\n",
    "        for track_id, box in assignments:\n",
    "            x1, y1, x2, y2 = [int(v) for v in box]\n",
    "            crop = pil.crop((x1, y1, x2, y2))\n",
    "            crop_for_blip = crop.resize((384, 384), Image.LANCZOS)\n",
    "\n",
    "            if track_id not in track_records:\n",
    "                try:\n",
    "                    caption = captioner.caption(crop_for_blip)\n",
    "                except Exception as e:\n",
    "                    caption = \"\"\n",
    "                    print(\"Caption error:\", e)\n",
    "                try:\n",
    "                    attributes = attr_matcher.get_attributes(crop_for_blip, top_k=ATTR_TOP_K)\n",
    "                except Exception as e:\n",
    "                    attributes = {}\n",
    "                    print(\"Attribute matcher error:\", e)\n",
    "\n",
    "                fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "                first_crop_fname = f\"cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}_first.jpg\"\n",
    "                first_crop_path = os.path.join(OUTPUT_CROPS_FOLDER, first_crop_fname)\n",
    "                try:\n",
    "                    crop.save(first_crop_path, format=\"JPEG\", quality=90)\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to save first crop:\", e)\n",
    "\n",
    "                track_records[track_id] = {\n",
    "                    \"track_id\": track_id,\n",
    "                    \"timestamp_start\": timestamp,\n",
    "                    \"timestamp_end\": timestamp,\n",
    "                    \"first_crop_path\": first_crop_path,\n",
    "                    \"last_crop_path\": first_crop_path,\n",
    "                    \"description\": caption,\n",
    "                    \"attributes\": attributes,\n",
    "                    \"bbox_start\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                    \"bbox_end\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n",
    "                }\n",
    "            else:\n",
    "                fname_ts = timestamp.replace(\":\", \"\").replace(\"-\", \"\")\n",
    "                last_crop_fname = f\"cam{CAMERA_ID}_trk{track_id}_{frame_base}_{fname_ts}_last.jpg\"\n",
    "                last_crop_path = os.path.join(OUTPUT_CROPS_FOLDER, last_crop_fname)\n",
    "                try:\n",
    "                    crop.save(last_crop_path, format=\"JPEG\", quality=90)\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to save last crop:\", e)\n",
    "\n",
    "                track_records[track_id][\"timestamp_end\"] = timestamp\n",
    "                track_records[track_id][\"last_crop_path\"] = last_crop_path\n",
    "                track_records[track_id][\"bbox_end\"] = {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2}\n",
    "\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"Processed {frame_idx}/{len(frame_files)} frames\")\n",
    "\n",
    "    # Save records\n",
    "    for track_id, rec in track_records.items():\n",
    "        record_fname = f\"record_cam{CAMERA_ID}_trk{track_id}.json\"\n",
    "        record_path = os.path.join(OUTPUT_RECORDS_FOLDER, record_fname)\n",
    "        try:\n",
    "            with open(record_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                json.dump(rec, fh, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to write record:\", e)\n",
    "\n",
    "    print(\"Done. Crops saved to:\", OUTPUT_CROPS_FOLDER)\n",
    "    print(\"Records saved to:\", OUTPUT_RECORDS_FOLDER)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline_single_record()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e51c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
